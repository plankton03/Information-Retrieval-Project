{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9-CpPl5htiUm",
        "outputId": "6e4e217f-cc32-4f15-a07b-428ea8768d5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: parsivar in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.3.1)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: nltk>=3.6.6 in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from parsivar) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.6.6->parsivar) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.6.6->parsivar) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.6.6->parsivar) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.6.6->parsivar) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\fatemeh abdi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk>=3.6.6->parsivar) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install parsivar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZHkZCoftp4X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from parsivar import FindStems\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import re\n",
        "from re import sub\n",
        "import time\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_CXmWuPuLx3"
      },
      "source": [
        "# Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oRxwOYCxuKUY"
      },
      "outputs": [],
      "source": [
        "json_file_path = 'IR_data_news_12k.json'\n",
        "\n",
        "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract content and use doc index as doc_id\n",
        "documents = [data[i]['content'] for i in data]\n",
        "urls = [data[i]['url'] for i in data]\n",
        "titles = [data[i]['title'] for i in data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-aiF-fbuUZY"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Difining normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyNormalizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        with open('verbs.json', 'r') as file:\n",
        "            self.verbs = json.load(file)\n",
        "        with open('words.json', 'r') as file:\n",
        "            self.words = json.load(file)\n",
        "\n",
        "\n",
        "    def normalize_alphabets(self,doc_string):\n",
        "        a0 = \"ء\"\n",
        "        b0 = \"ئ\"\n",
        "        c0 = sub(a0, b0, doc_string)\n",
        "        a1 = r\"ٲ|ٱ|إ|ﺍ|أ\"\n",
        "        a11 = r\"ﺁ|آ\"\n",
        "        b1 = r\"ا\"\n",
        "        b11 = r\"آ\"\n",
        "        c11 = sub(a11, b11, c0)\n",
        "        c1 = sub(a1, b1, c11)\n",
        "        a2 = r\"ﺐ|ﺏ|ﺑ\"\n",
        "        b2 = r\"ب\"\n",
        "        c2 = sub(a2, b2, c1)\n",
        "        a3 = r\"ﭖ|ﭗ|ﭙ|ﺒ|ﭘ\"\n",
        "        b3 = r\"پ\"\n",
        "        c3 = sub(a3, b3, c2)\n",
        "        a4 = r\"ﭡ|ٺ|ٹ|ﭞ|ٿ|ټ|ﺕ|ﺗ|ﺖ|ﺘ\"\n",
        "        b4 = r\"ت\"\n",
        "        c4 = sub(a4, b4, c3)\n",
        "        a5 = r\"ﺙ|ﺛ\"\n",
        "        b5 = r\"ث\"\n",
        "        c5 = sub(a5, b5, c4)\n",
        "        a6 = r\"ﺝ|ڃ|ﺠ|ﺟ\"\n",
        "        b6 = r\"ج\"\n",
        "        c6 = sub(a6, b6, c5)\n",
        "        a7 = r\"ڃ|ﭽ|ﭼ\"\n",
        "        b7 = r\"چ\"\n",
        "        c7 = sub(a7, b7, c6)\n",
        "        a8 = r\"ﺢ|ﺤ|څ|ځ|ﺣ\"\n",
        "        b8 = r\"ح\"\n",
        "        c8 = sub(a8, b8, c7)\n",
        "        a9 = r\"ﺥ|ﺦ|ﺨ|ﺧ\"\n",
        "        b9 = r\"خ\"\n",
        "        c9 = sub(a9, b9, c8)\n",
        "        a10 = r\"ڏ|ډ|ﺪ|ﺩ\"\n",
        "        b10 = r\"د\"\n",
        "        c10 = sub(a10, b10, c9)\n",
        "        a11 = r\"ﺫ|ﺬ|ﻧ\"\n",
        "        b11 = r\"ذ\"\n",
        "        c11 = sub(a11, b11, c10)\n",
        "        a12 = r\"ڙ|ڗ|ڒ|ڑ|ڕ|ﺭ|ﺮ\"\n",
        "        b12 = r\"ر\"\n",
        "        c12 = sub(a12, b12, c11)\n",
        "        a13 = r\"ﺰ|ﺯ\"\n",
        "        b13 = r\"ز\"\n",
        "        c13 = sub(a13, b13, c12)\n",
        "        a14 = r\"ﮊ\"\n",
        "        b14 = r\"ژ\"\n",
        "        c14 = sub(a14, b14, c13)\n",
        "        a15 = r\"ݭ|ݜ|ﺱ|ﺲ|ښ|ﺴ|ﺳ\"\n",
        "        b15 = r\"س\"\n",
        "        c15 = sub(a15, b15, c14)\n",
        "        a16 = r\"ﺵ|ﺶ|ﺸ|ﺷ\"\n",
        "        b16 = r\"ش\"\n",
        "        c16 = sub(a16, b16, c15)\n",
        "        a17 = r\"ﺺ|ﺼ|ﺻ\"\n",
        "        b17 = r\"ص\"\n",
        "        c17 = sub(a17, b17, c16)\n",
        "        a18 = r\"ﺽ|ﺾ|ﺿ|ﻀ\"\n",
        "        b18 = r\"ض\"\n",
        "        c18 = sub(a18, b18, c17)\n",
        "        a19 = r\"ﻁ|ﻂ|ﻃ|ﻄ\"\n",
        "        b19 = r\"ط\"\n",
        "        c19 = sub(a19, b19, c18)\n",
        "        a20 = r\"ﻆ|ﻇ|ﻈ\"\n",
        "        b20 = r\"ظ\"\n",
        "        c20 = sub(a20, b20, c19)\n",
        "        a21 = r\"ڠ|ﻉ|ﻊ|ﻋ\"\n",
        "        b21 = r\"ع\"\n",
        "        c21 = sub(a21, b21, c20)\n",
        "        a22 = r\"ﻎ|ۼ|ﻍ|ﻐ|ﻏ\"\n",
        "        b22 = r\"غ\"\n",
        "        c22 = sub(a22, b22, c21)\n",
        "        a23 = r\"ﻒ|ﻑ|ﻔ|ﻓ\"\n",
        "        b23 = r\"ف\"\n",
        "        c23 = sub(a23, b23, c22)\n",
        "        a24 = r\"ﻕ|ڤ|ﻖ|ﻗ\"\n",
        "        b24 = r\"ق\"\n",
        "        c24 = sub(a24, b24, c23)\n",
        "        a25 = r\"ڭ|ﻚ|ﮎ|ﻜ|ﮏ|ګ|ﻛ|ﮑ|ﮐ|ڪ|ك\"\n",
        "        b25 = r\"ک\"\n",
        "        c25 = sub(a25, b25, c24)\n",
        "        a26 = r\"ﮚ|ﮒ|ﮓ|ﮕ|ﮔ\"\n",
        "        b26 = r\"گ\"\n",
        "        c26 = sub(a26, b26, c25)\n",
        "        a27 = r\"ﻝ|ﻞ|ﻠ|ڵ\"\n",
        "        b27 = r\"ل\"\n",
        "        c27 = sub(a27, b27, c26)\n",
        "        a28 = r\"ﻡ|ﻤ|ﻢ|ﻣ\"\n",
        "        b28 = r\"م\"\n",
        "        c28 = sub(a28, b28, c27)\n",
        "        a29 = r\"ڼ|ﻦ|ﻥ|ﻨ\"\n",
        "        b29 = r\"ن\"\n",
        "        c29 = sub(a29, b29, c28)\n",
        "        a30 = r\"ވ|ﯙ|ۈ|ۋ|ﺆ|ۊ|ۇ|ۏ|ۅ|ۉ|ﻭ|ﻮ|ؤ\"\n",
        "        b30 = r\"و\"\n",
        "        c30 = sub(a30, b30, c29)\n",
        "        a31 = r\"ﺔ|ﻬ|ھ|ﻩ|ﻫ|ﻪ|ۀ|ە|ة|ہ\"\n",
        "        b31 = r\"ه\"\n",
        "        c31 = sub(a31, b31, c30)\n",
        "        a32 = r\"ﭛ|ﻯ|ۍ|ﻰ|ﻱ|ﻲ|ں|ﻳ|ﻴ|ﯼ|ې|ﯽ|ﯾ|ﯿ|ێ|ے|ى|ي\"\n",
        "        b32 = r\"ی\"\n",
        "        c32 = sub(a32, b32, c31)\n",
        "        a33 = r'¬'\n",
        "        b33 = r'‌'\n",
        "        c33 = sub(a33, b33, c32)\n",
        "        pa0 = r'•|·|●|·|・|∙|｡|ⴰ'\n",
        "        pb0 = r'.'\n",
        "        pc0 = sub(pa0, pb0, c33)\n",
        "        pa1 = r',|٬|٫|‚|，'\n",
        "        pb1 = r'،'\n",
        "        pc1 = sub(pa1, pb1, pc0)\n",
        "        pa2 = r'ʕ'\n",
        "        pb2 = r'؟'\n",
        "        pc2 = sub(pa2, pb2, pc1)\n",
        "        na0 = r'0|٠'\n",
        "        nb0 = r'۰'\n",
        "        nc0 = sub(na0, nb0, pc2)\n",
        "        na1 = r'1|١'\n",
        "        nb1 = r'۱'\n",
        "        nc1 = sub(na1, nb1, nc0)\n",
        "        na2 = r'2|٢'\n",
        "        nb2 = r'۲'\n",
        "        nc2 = sub(na2, nb2, nc1)\n",
        "        na3 = r'3|٣'\n",
        "        nb3 = r'۳'\n",
        "        nc3 = sub(na3, nb3, nc2)\n",
        "        na4 = r'4|٤'\n",
        "        nb4 = r'۴'\n",
        "        nc4 = sub(na4, nb4, nc3)\n",
        "        na5 = r'5'\n",
        "        nb5 = r'۵'\n",
        "        nc5 = sub(na5, nb5, nc4)\n",
        "        na6 = r'۶|٦'\n",
        "        nb6 = r'6'\n",
        "        nc6 = sub(na6, nb6, nc5)\n",
        "        na7 = r'7|٧'\n",
        "        nb7 = r'۷'\n",
        "        nc7 = sub(na7, nb7, nc6)\n",
        "        na8 = r'8|٨'\n",
        "        nb8 = r'۸'\n",
        "        nc8 = sub(na8, nb8, nc7)\n",
        "        na9 = r'9|٩'\n",
        "        nb9 = r'۹'\n",
        "        nc9 = sub(na9, nb9, nc8)\n",
        "        ea1 = r'ـ|ِ|ُ|َ|ٍ|ٌ|ً|'\n",
        "        eb1 = r''\n",
        "        ec1 = sub(ea1, eb1, nc9)\n",
        "        Sa1 = r'( )+'\n",
        "        Sb1 = r' '\n",
        "        Sc1 = sub(Sa1, Sb1, ec1)\n",
        "        Sa2 = r'(\\n)+'\n",
        "        Sb2 = r'\\n'\n",
        "        Sc2 = sub(Sa2, Sb2, Sc1)\n",
        "        return Sc2\n",
        "\n",
        "\n",
        "    def normalize_unicodes(self,text):\n",
        "        unicodes = [(\"﷽\", \"بسم الله الرحمن الرحیم\"),\n",
        "                    (\"﷼\", \"ریال\"),\n",
        "                    (\"(ﷰ|ﷹ)\", \"صلی\"),\n",
        "                    (\"ﷲ\", \"الله\"),\n",
        "                    (\"ﷳ\", \"اکبر\"),\n",
        "                    (\"ﷴ\", \"محمد\"),\n",
        "                    (\"ﷵ\", \"صلعم\"),\n",
        "                    (\"ﷶ\", \"رسول\"),\n",
        "                    (\"ﷷ\", \"علیه\"),\n",
        "                    (\"ﷸ\", \"وسلم\"),\n",
        "                    (\"ﻵ|ﻶ|ﻷ|ﻸ|ﻹ|ﻺ|ﻻ|ﻼ\", \"لا\")]\n",
        "        \n",
        "        for old, new in unicodes:\n",
        "            text = sub(old,new,text)\n",
        "            return text\n",
        "\n",
        "\n",
        "    def normalize_special_characters(self,text):\n",
        "\n",
        "        removed_unnecessary_dots_text = re.sub(r'(?<!\\d)\\.|\\.(?!\\d)', '', text)\n",
        "\n",
        "        cleaned_text = re.sub(r'[^\\w\\s\\d@_.-]', '', removed_unnecessary_dots_text)\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "\n",
        "    def simple_tokenizer(self,text):\n",
        "        \n",
        "        split_pattern = r'[ \\t\\u200c]+'\n",
        "\n",
        "        tokens = re.split(split_pattern, text)\n",
        "\n",
        "        tokens = [token for token in tokens if token]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def fix_extra_spaces(self,text):\n",
        "\n",
        "        extra_space_patterns = [\n",
        "                  (r\" {2,}\", \" \"),  # remove extra spaces\n",
        "                  (r\"\\n{3,}\", \"\\n\\n\"),  # remove extra newlines\n",
        "                  (r\"\\u200c{2,}\", \"\\u200c\"),  # remove extra ZWNJs\n",
        "                  (r\"\\u200c{1,} \", \" \"),  # remove unneded ZWNJs before space\n",
        "                  (r\" \\u200c{1,}\", \" \"),  # remove unneded ZWNJs after space\n",
        "                  (r\"\\b\\u200c*\\B\", \"\"),  # remove unneded ZWNJs at the beginning of words\n",
        "                  (r\"\\B\\u200c*\\b\", \"\"),  # remove unneded ZWNJs at the end of words\n",
        "                  (r\"[ـ\\r]\", \"\"),  # remove keshide, carriage returns\n",
        "              ]\n",
        "        for old, new in extra_space_patterns:\n",
        "            text = sub(old,new,text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def fix_number_spaces(self, text):\n",
        "\n",
        "        number_space_patterns = [(r\"(\\d)([آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی])\", r\"\\1 \\2\"),\n",
        "                  (r\"([آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی])(\\d)\", r\"\\1 \\2\"),\n",
        "              ]\n",
        "        for old, new in number_space_patterns:\n",
        "            text = sub(old,new,text)\n",
        "\n",
        "        return text\n",
        "    def fix_affix_spaces(self,text):\n",
        "\n",
        "        affix_spacing_patterns = [\n",
        "                (r\"([^ ]ه) ی \", r\"\\1‌ی \"),  # fix ی space\n",
        "                (r\"(^| )(ن?می) \", r\"\\1\\2‌\"),  # put zwnj after می, نمی\n",
        "                # put zwnj before تر, تری, ترین, گر, گری, ها, های\n",
        "                (\n",
        "                    r\"(?<=[^\\n\\d \"\n",
        "                    + \"]{2}) (تر(ین?)?|گری?|های?)(?=[ \\n\"\n",
        "                    + \"]|$)\",\n",
        "                    r\"‌\\1\",\n",
        "                ),\n",
        "                # join ام, ایم, اش, اند, ای, اید, ات\n",
        "                (\n",
        "                    r\"([^ ]ه) (ا(م|یم|ش|ند|ی|ید|ت))(?=[ \\n\" + \"]|$)\",\n",
        "                    r\"\\1‌\\2\",\n",
        "                ),\n",
        "                # شنبهها => شنبه‌ها\n",
        "                (\"(ه)(ها)\", r\"\\1‌\\2\"),\n",
        "            ]\n",
        "        \n",
        "        for old, new in affix_spacing_patterns:\n",
        "            text = sub(old,new,text)\n",
        "        return text\n",
        "    \n",
        "\n",
        "    def correct_spacing(self,text):\n",
        "\n",
        "        text = self.fix_extra_spaces(text)\n",
        "        \n",
        "        text = self.fix_number_spaces(text)\n",
        "\n",
        "        lines = text.split(\"\\n\")\n",
        "        result = []\n",
        "        for line in lines:\n",
        "            tokens = self.simple_tokenizer(line)\n",
        "            spaced_tokens = self.token_spacing(tokens)\n",
        "            line = \" \".join(spaced_tokens)\n",
        "            result.append(line)\n",
        "\n",
        "        text = \"\\n\".join(result)\n",
        "        \n",
        "        text = self.fix_affix_spaces(text)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def token_spacing(self,tokens):\n",
        "\n",
        "        suffixes = {\n",
        "                \"ی\",\n",
        "                \"ای\",\n",
        "                \"ها\",\n",
        "                \"های\",\n",
        "                \"هایی\",\n",
        "                \"تر\",\n",
        "                \"تری\",\n",
        "                \"ترین\",\n",
        "                \"گر\",\n",
        "                \"گری\",\n",
        "                \"ام\",\n",
        "                \"ات\",\n",
        "                \"اش\",\n",
        "            }\n",
        "        \n",
        "        result = []\n",
        "        \n",
        "        for t, token in enumerate(tokens):\n",
        "            joined = False\n",
        "\n",
        "            if result:\n",
        "                token_pair = result[-1] + \"‌\" + token\n",
        "                if (\n",
        "                    token_pair in self.verbs\n",
        "                    or token_pair in self.words\n",
        "                    and self.words[token_pair][0] > 0\n",
        "                ):\n",
        "                    joined = True\n",
        "\n",
        "                    if (\n",
        "                        t < len(tokens) - 1\n",
        "                        and token + \"_\" + tokens[t + 1] in self.verbs\n",
        "                    ):\n",
        "                        joined = False\n",
        "\n",
        "                elif token in suffixes and result[-1] in self.words:\n",
        "                    joined = True\n",
        "\n",
        "            if joined:\n",
        "                result.pop()\n",
        "                result.append(token_pair)\n",
        "            else:\n",
        "                result.append(token)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def seperate_mi(self,text):\n",
        "\n",
        "        matches = re.findall(r\"\\bن?می[آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی]+\", text)\n",
        "        for m in matches:\n",
        "            r = re.sub(\"^(ن?می)\", r\"\\1‌\", m)\n",
        "            if r in self.verbs:\n",
        "                text = text.replace(m, r)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def normalize(self,text):\n",
        "        text = self.normalize_alphabets(text)\n",
        "        text = self.normalize_unicodes(text)\n",
        "        text = self.normalize_special_characters(text)\n",
        "        text = self.correct_spacing( text)\n",
        "        text = self.seperate_mi(text)\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "به گزارش خبرگزاری فارس کنفدراسیون فوتبال آسیا AFC در نامه‌ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان  قرعه‌کشی جام باشگاه‌های فوتسال آسیا را رسما اعلام کرد بر این اساس ۲۵ فروردین‌ماه ۱۴۰۱ مراسم قرعه‌کشی جام باشگاه‌های فوتسال آسیا در مالزی برگزار می‌شود باشگاه گیتی پسند بعنوان قهرمان فوتسال ایران در سال ۱۴۰۰ به این مسابقات راه پیدا کرده است پیش از این گیتی پسند تجربه ۳ دوره حضور در جام باشگاه‌های فوتسال آسیا را داشته که هر سه دوره به فینال مسابقات راه پیدا کرده و یک عنوان قهرمانی و دو مقام دومی بدست آورده است انتهای پیام\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text = documents[0]\n",
        "my_normalizer = MyNormalizer()\n",
        "print(my_normalizer.normalize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['من', 'فاطمه', 'عبدی', 'با', 'شماره', 'دانشجویی', '9931089', 'و', 'ایمیل', 'fatemehabdina@gmail.com', 'و', 'آیدی', '@fatem_abdi', 'هستم']\n"
          ]
        }
      ],
      "source": [
        "def tokenize_text(text,my_normalizer):\n",
        "    # Regular expression patterns for identifying emails and numbers\n",
        "    # email_pattern = re.compile(r\"[a-zA-Z0-9._+-]+@([a-zA-Z0-9-]+\\.)+[A-Za-z]{2,}\",)\n",
        "    # number_int_pattern = re.compile(r\"\\b(?<![\\d۰-۹][.٫٬,])([\\d۰-۹]+)(?![.٫٬,][\\d۰-۹])\\b\",)\n",
        "    # number_float_pattern = re.compile(r\"\\b(?<!\\.)([\\d۰-۹,٬]+[.٫٬][\\d۰-۹]+)\\b(?!\\.)\",)\n",
        "    # id_pattern = re.compile(r\"(?<![\\w._])(@[\\w_]+)\")\n",
        "\n",
        "    # Combine the email and number patterns into one pattern\n",
        "    # combined_pattern = f'({email_pattern})|({number_int_pattern})|({number_float_pattern})|({id_pattern})'\n",
        "\n",
        "    tokens = re.split(r'\\t|\\n| |‌', text)\n",
        "\n",
        "    tokens = [token for token in tokens if token]\n",
        "    \n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    text = my_normalizer.correct_spacing(text)\n",
        "\n",
        "    words = re.split(r' ', text)\n",
        "\n",
        "    # Tokenize the words using the combined pattern\n",
        "    # tokens = [re.search(combined_pattern, word).group() if re.search(combined_pattern, word) else word for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Example usage\n",
        "text_to_tokenize = \"من فاطمه عبدی با شماره دانشجویی9931089 و ایمیل fatemehabdina@gmail.com و آیدی @fatem_abdi هستم\"\n",
        "tokens = tokenize_text(text_to_tokenize, my_normalizer)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing, tokenizing, stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vDgnaQrcuZKZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "stemmer = FindStems()\n",
        "\n",
        "\n",
        "preprocessed_tokens = []\n",
        "\n",
        "for content in documents:\n",
        "\n",
        "    normalized_content = my_normalizer.normalize(content)\n",
        "\n",
        "    content_tokens = tokenize_text(normalized_content,my_normalizer=my_normalizer)\n",
        "\n",
        "    stemmed_tokens = [stemmer.convert_to_stem(token) for token in content_tokens]\n",
        "\n",
        "    preprocessed_tokens.append(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP6ht7ypvWPx"
      },
      "source": [
        "### Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nIDeBZWDvcoV"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_frequencies = {}\n",
        "for tokens in preprocessed_tokens:\n",
        "    for token in tokens:\n",
        "        word_frequencies[token] = word_frequencies.get(token, 0) + 1\n",
        "\n",
        "# Sort words based on their frequencies\n",
        "sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Select the top 50 most frequent words to remove\n",
        "stop_words = [word for word, frequency in sorted_words[:50]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0xJLaKZUxnAX"
      },
      "outputs": [],
      "source": [
        "# Remove the selected words from the documents\n",
        "final_tokens = [ [token for token in tokens if token not in stop_words] for tokens in preprocessed_tokens]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBs40npVxoum",
        "outputId": "73434899-ef5b-42bc-8421-ed905fc2651b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List of stopwords and their frequencies:\n",
            "1- و: 218109\n",
            "2- در: 163370\n",
            "3- به: 132417\n",
            "4- از: 92525\n",
            "5- این: 81940\n",
            "6- که: 75067\n",
            "7- با: 68524\n",
            "8- را: 67199\n",
            "9- اس: 56763\n",
            "10- کرد&کن: 44429\n",
            "11- برای: 30913\n",
            "12- داشت&دار: 30598\n",
            "13- شد&شو: 28741\n",
            "14- تیم: 27477\n",
            "15- کرد: 24111\n",
            "16- بود&باش: 23918\n",
            "17- هم: 21656\n",
            "18- کشور: 21544\n",
            "19- ما: 19612\n",
            "20- شد: 18970\n",
            "21- بازی: 17692\n",
            "22- یک: 17559\n",
            "23- آن: 16667\n",
            "24- باید: 16107\n",
            "25- تا: 15808\n",
            "26- بر: 15622\n",
            "27- وی: 15345\n",
            "28- داد&ده: 14620\n",
            "29- خود: 14494\n",
            "30- خواست&خواه: 14380\n",
            "31- مجلس: 14290\n",
            "32- اسلامی: 14241\n",
            "33- گزارش: 13909\n",
            "34- گفت: 13303\n",
            "35- فارس: 13245\n",
            "36- مردم: 13046\n",
            "37- ایران: 12501\n",
            "38- خبرگزاری: 12399\n",
            "39- پیام: 12252\n",
            "40- سال: 12212\n",
            "41- دولت: 12165\n",
            "42- انتهای: 12104\n",
            "43- اما: 12026\n",
            "44- گرفت&گیر: 11376\n",
            "45- توانست&توان: 11063\n",
            "46- بازیکن: 10980\n",
            "47- داشت&دارد: 10568\n",
            "48- اینکه: 10106\n",
            "49- ملی: 9762\n",
            "50- کار: 9638\n"
          ]
        }
      ],
      "source": [
        "print(\"List of stopwords and their frequencies:\")\n",
        "for i , word in enumerate(stop_words):\n",
        "    print(f\"{i+1}- {word}: {word_frequencies[word]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Positional indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_mpYNv-gzYvt"
      },
      "outputs": [],
      "source": [
        "positional_index = defaultdict(lambda: {\"total_count\": 0, \"postings_lists\": defaultdict(lambda: {\"count\": 0, \"positions\": [],\"tf_idf\":0})})\n",
        "\n",
        "for doc_id, tokens in enumerate(final_tokens):\n",
        "    for position, token in enumerate(tokens):\n",
        "\n",
        "        positional_index[token][\"total_count\"] += 1\n",
        "\n",
        "        positional_index[token][\"postings_lists\"][doc_id][\"count\"] += 1\n",
        "        positional_index[token][\"postings_lists\"][doc_id][\"positions\"].append(position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Pa923p39z-Rl",
        "outputId": "9c11b4ea-677d-4a2f-8785-3334877d90e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 'امیرکبیر' appears 43 times in the entire document collection.\n",
            "Document frequency of 'امیرکبیر': 24 documents.\n",
            " - In document 7346, it appears 2 times at positions [7, 98]\n",
            " - In document 7395, it appears 1 times at positions [14]\n",
            " - In document 7416, it appears 1 times at positions [21]\n",
            " - In document 7579, it appears 1 times at positions [955]\n",
            " - In document 7634, it appears 1 times at positions [1888]\n",
            " - In document 8125, it appears 4 times at positions [19, 31, 59, 122]\n",
            " - In document 8463, it appears 2 times at positions [1032, 1068]\n",
            " - In document 8645, it appears 1 times at positions [16]\n",
            " - In document 9261, it appears 1 times at positions [38]\n",
            " - In document 9286, it appears 4 times at positions [852, 1027, 1063, 1285]\n",
            " - In document 9808, it appears 1 times at positions [70]\n",
            " - In document 9845, it appears 1 times at positions [1303]\n",
            " - In document 10024, it appears 2 times at positions [92, 115]\n",
            " - In document 10361, it appears 1 times at positions [67]\n",
            " - In document 10567, it appears 3 times at positions [153, 184, 203]\n",
            " - In document 10568, it appears 2 times at positions [643, 729]\n",
            " - In document 10681, it appears 1 times at positions [17]\n",
            " - In document 11176, it appears 1 times at positions [112]\n",
            " - In document 11405, it appears 3 times at positions [54, 1277, 1367]\n",
            " - In document 11437, it appears 1 times at positions [3119]\n",
            " - In document 11603, it appears 6 times at positions [14, 22, 50, 98, 171, 235]\n",
            " - In document 11645, it appears 1 times at positions [16]\n",
            " - In document 11694, it appears 1 times at positions [260]\n",
            " - In document 12134, it appears 1 times at positions [371]\n"
          ]
        }
      ],
      "source": [
        "# Example of how to access information in the positional index\n",
        "\n",
        "example_token = \"امیرکبیر\"\n",
        "total_count = positional_index[example_token][\"total_count\"]\n",
        "document_frequency = positional_index[example_token][\"postings_lists\"]\n",
        "\n",
        "print(f\"Token '{example_token}' appears {total_count} times in the entire document collection.\")\n",
        "print(f\"Document frequency of '{example_token}': {len(document_frequency)} documents.\")\n",
        "for doc_id, info in document_frequency.items():\n",
        "    print(f\" - In document {doc_id}, it appears {info['count']} times at positions {info['positions']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HMnruIL0lgN"
      },
      "source": [
        "# Vectorizing documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7VIai_y0Qn9",
        "outputId": "7406386a-68bb-42d7-9c2b-a1cc8ef8f709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12202\n"
          ]
        }
      ],
      "source": [
        "collection_size = len(final_tokens)\n",
        "print(collection_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K45Q9bJi1L1e"
      },
      "outputs": [],
      "source": [
        "# Update the positional index with TF-IDF values\n",
        "for term, docs_info in positional_index.items():\n",
        "    for doc_id, _ in docs_info[\"postings_lists\"].items():\n",
        "        tf_value = positional_index[term][\"postings_lists\"][doc_id][\"count\"]\n",
        "        idf_value = len(positional_index[term][\"postings_lists\"])\n",
        "        tfidf_value = (1 + math.log10(tf_value)) * math.log10(collection_size/idf_value)\n",
        "        positional_index[term][\"postings_lists\"][doc_id][\"tf_idf\"] = tfidf_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.520873107266285\n"
          ]
        }
      ],
      "source": [
        "print(positional_index['امیرکبیر'][\"postings_lists\"][7346][\"tf_idf\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7QAdwxw8cOc"
      },
      "source": [
        "# Create Champion lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "t-I95h3N7Yh0"
      },
      "outputs": [],
      "source": [
        "def create_champion_list(term, k):\n",
        "\n",
        "    postings_list = positional_index[term][\"postings_lists\"]\n",
        "\n",
        "    sorted_postings = sorted(postings_list.items(), key=lambda x: x[1][\"tf_idf\"]/len(final_tokens[x[0]]), reverse=True)\n",
        "\n",
        "    champion_list = sorted_postings[:k]\n",
        "\n",
        "    return champion_list\n",
        "\n",
        "def create_all_champion_lists(k):\n",
        "    champion_lists = {}\n",
        "\n",
        "    for term in positional_index.keys():\n",
        "        champion_lists[term] = create_champion_list(term, k)\n",
        "\n",
        "    return champion_lists\n",
        "\n",
        "champion_lists = create_all_champion_lists(150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROiSCV7M9f70",
        "outputId": "fb857ff5-e947-45e2-8b47-019cd8a5498d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(10361, {'count': 1, 'positions': [67], 'tf_idf': 2.706219778944763}), (9808, {'count': 1, 'positions': [70], 'tf_idf': 2.706219778944763}), (8125, {'count': 4, 'positions': [19, 31, 59, 122], 'tf_idf': 4.335526435587807}), (11603, {'count': 6, 'positions': [14, 22, 50, 98, 171, 235], 'tf_idf': 4.812068083743577}), (10567, {'count': 3, 'positions': [153, 184, 203], 'tf_idf': 3.9974147554220556}), (10024, {'count': 2, 'positions': [92, 115], 'tf_idf': 3.520873107266285}), (9261, {'count': 1, 'positions': [38], 'tf_idf': 2.706219778944763}), (8645, {'count': 1, 'positions': [16], 'tf_idf': 2.706219778944763}), (11645, {'count': 1, 'positions': [16], 'tf_idf': 2.706219778944763}), (7346, {'count': 2, 'positions': [7, 98], 'tf_idf': 3.520873107266285}), (12134, {'count': 1, 'positions': [371], 'tf_idf': 2.706219778944763}), (7395, {'count': 1, 'positions': [14], 'tf_idf': 2.706219778944763}), (7416, {'count': 1, 'positions': [21], 'tf_idf': 2.706219778944763}), (10681, {'count': 1, 'positions': [17], 'tf_idf': 2.706219778944763}), (11694, {'count': 1, 'positions': [260], 'tf_idf': 2.706219778944763}), (10568, {'count': 2, 'positions': [643, 729], 'tf_idf': 3.520873107266285}), (8463, {'count': 2, 'positions': [1032, 1068], 'tf_idf': 3.520873107266285}), (9286, {'count': 4, 'positions': [852, 1027, 1063, 1285], 'tf_idf': 4.335526435587807}), (11405, {'count': 3, 'positions': [54, 1277, 1367], 'tf_idf': 3.9974147554220556}), (11176, {'count': 1, 'positions': [112], 'tf_idf': 2.706219778944763}), (7579, {'count': 1, 'positions': [955], 'tf_idf': 2.706219778944763}), (9845, {'count': 1, 'positions': [1303], 'tf_idf': 2.706219778944763}), (7634, {'count': 1, 'positions': [1888], 'tf_idf': 2.706219778944763}), (11437, {'count': 1, 'positions': [3119], 'tf_idf': 2.706219778944763})]\n"
          ]
        }
      ],
      "source": [
        "print(champion_lists['امیرکبیر'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv5X2llD2B-F"
      },
      "source": [
        "# Query Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk-IiPro3am9"
      },
      "source": [
        "## Query preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LEkJ1iki2AY-"
      },
      "outputs": [],
      "source": [
        "def query_preprocess(query):\n",
        "    normalized_query = my_normalizer.normalize(query)\n",
        "    query_tokens = tokenize_text(normalized_query,my_normalizer)\n",
        "    stemmed_query = [stemmer.convert_to_stem(token) for token in query_tokens]\n",
        "    final_query = [token for token in stemmed_query if token not in stop_words]\n",
        "    return final_query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrYMnCQH3669"
      },
      "source": [
        "## Check for query validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3P8lktSM36nl"
      },
      "outputs": [],
      "source": [
        "def is_query_valid(preprocessed_query):\n",
        "    if len(preprocessed_query) == 0:\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcp7IBOQ4xGT"
      },
      "source": [
        "## Calculate tf for query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-lzAZLfH5JPd"
      },
      "outputs": [],
      "source": [
        "def calculate_tf_query(preprocessed_query):\n",
        "    tf = {}\n",
        "    for term in preprocessed_query:\n",
        "        tf[term] = tf.get(term, 0) + 1\n",
        "    for term,frequency in tf.items():\n",
        "        # print(term, tf[term])\n",
        "        tf[term] =(1 + math.log(tf[term]))\n",
        "        # print(term, tf[term])\n",
        "\n",
        "    return tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEU6PCjw5eBP"
      },
      "source": [
        "## Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-wGu5gAm5c_W"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(query_with_tf,tokens,use_champion_list):\n",
        "    multiplied_tfidf_docs = defaultdict(lambda: 0)\n",
        "    if(use_champion_list == False):\n",
        "        for term,tf_term in query_with_tf.items():\n",
        "            if(term not in champion_lists.keys()):\n",
        "                continue\n",
        "            for doc_id, term_doc_info in positional_index[term][\"postings_lists\"].items():\n",
        "                mul_tf = tf_term * positional_index[term][\"postings_lists\"][doc_id][\"tf_idf\"]\n",
        "                multiplied_tfidf_docs[doc_id] = multiplied_tfidf_docs[doc_id] + mul_tf\n",
        "    else:\n",
        "        for term,tf_term in query_with_tf.items():\n",
        "            if(term not in champion_lists.keys()):\n",
        "                continue\n",
        "            for doc in champion_lists[term]:\n",
        "                doc_id = doc[0]\n",
        "                tf_idf_doc = doc[1][\"tf_idf\"]\n",
        "                mul_tf = tf_term * tf_idf_doc\n",
        "                multiplied_tfidf_docs[doc_id] = multiplied_tfidf_docs[doc_id] + mul_tf\n",
        "\n",
        "\n",
        "    for doc_id, mul_tf in multiplied_tfidf_docs.items():\n",
        "        len_doc = len(tokens[doc_id])\n",
        "        multiplied_tfidf_docs[doc_id] /= (len_doc*len(query_with_tf))\n",
        "\n",
        "    return multiplied_tfidf_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyanwydVASIr"
      },
      "source": [
        "## Ranked retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "avQ4u0TsCOUi"
      },
      "outputs": [],
      "source": [
        "def top_k_docs(query_tf,k):\n",
        "    cosine_similarities =  cosine_similarity(query_tf,final_tokens,True)\n",
        "    if(len(cosine_similarities) < k):\n",
        "        cosine_similarities =  cosine_similarity(query_tf,final_tokens,False)\n",
        "        sorted_cosine_similarities = sorted(cosine_similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "        return sorted_cosine_similarities\n",
        "    else:\n",
        "        sorted_cosine_similarities = sorted(cosine_similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "        return sorted_cosine_similarities[:k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "A4drn1lFDawA"
      },
      "outputs": [],
      "source": [
        "urls =[data[i]['url'] for i in data]\n",
        "titles = [data[i]['title'] for i in data]\n",
        "def show_results(query,top_answers):\n",
        "\n",
        "    print(f\"Query: {query}\\nDocuments:\\n\")\n",
        "\n",
        "    for doc_id, cosine_sim in top_answers:\n",
        "        print(f\"  Document ID: {doc_id}\")\n",
        "        print(f\"    URL: {urls[doc_id]}\")\n",
        "        print(f\"    Title: {titles[doc_id]}\")\n",
        "        print(f\"    Cosine similarity: {cosine_sim}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rb8c2AJK_1ak"
      },
      "outputs": [],
      "source": [
        "def execute_search_engine(query, number_of_retrieved_docs):\n",
        "    preprocessed_query = query_preprocess(query)\n",
        "    query_is_valid = is_query_valid(preprocessed_query)\n",
        "    if(query_is_valid == False):\n",
        "        print(\"Need more details\")\n",
        "        return\n",
        "    query_tf = calculate_tf_query(preprocessed_query)\n",
        "    top_answers = top_k_docs(query_tf,number_of_retrieved_docs)\n",
        "\n",
        "    show_results(query,top_answers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Need more details\n"
          ]
        }
      ],
      "source": [
        "query = 'ایران'\n",
        "k = 5\n",
        "execute_search_engine(query,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Need more details\n"
          ]
        }
      ],
      "source": [
        "query = 'فارس'\n",
        "k = 5\n",
        "execute_search_engine(query,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: اخبار\n",
            "Documents:\n",
            "\n",
            "  Document ID: 11168\n",
            "    URL: https://www.farsnews.ir/news/14000820000673/مدیر-اخبار-دفتر-رئیس-جمهور-منصوب-شد\n",
            "    Title: مدیر اخبار دفتر رئیس جمهور منصوب شد\n",
            "    Cosine similarity: 0.06896630021727439\n",
            "  Document ID: 3439\n",
            "    URL: https://www.farsnews.ir/news/14001108000590/کاپیتان-تراکتور-به-تیم-ملی-دعوت-شد\n",
            "    Title: کاپیتان تراکتور به تیم ملی دعوت شد\n",
            "    Cosine similarity: 0.04486361263632067\n",
            "  Document ID: 7027\n",
            "    URL: https://www.farsnews.ir/news/14001219000246/اعضای-مجلس-خبرگان-با-رهبر-انقلاب-دیدار-کردند\n",
            "    Title: اعضای مجلس خبرگان با رهبر انقلاب دیدار کردند\n",
            "    Cosine similarity: 0.04388764559281098\n",
            "  Document ID: 8453\n",
            "    URL: https://www.farsnews.ir/news/14001104000352/رئیس‌جمهور-به‌-صورت-زنده-با-مردم-صحبت-می‌کند\n",
            "    Title: رئیس‌جمهور به‌ صورت زنده با مردم صحبت می‌کند\n",
            "    Cosine similarity: 0.04388764559281098\n",
            "  Document ID: 8233\n",
            "    URL: https://www.farsnews.ir/news/14001110000183/جمعی-از-تولیدکنندگان-با-رهبر-انقلاب-دیدار-کردند\n",
            "    Title: جمعی از تولیدکنندگان با رهبر انقلاب دیدار کردند\n",
            "    Cosine similarity: 0.04197948708877572\n"
          ]
        }
      ],
      "source": [
        "query = 'اخبار'\n",
        "k = 5\n",
        "execute_search_engine(query,k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: استقلال و پرسپولیس\n",
            "Documents:\n",
            "\n",
            "  Document ID: 2754\n",
            "    URL: https://www.farsnews.ir/news/14001117000919/بازیکن-سابق-پرسپولیس-و-استقلال-به-پدیده-پیوست\n",
            "    Title: بازیکن سابق پرسپولیس و استقلال به پدیده پیوست\n",
            "    Cosine similarity: 0.04365571007008382\n",
            "  Document ID: 5437\n",
            "    URL: https://www.farsnews.ir/news/14001014000922/برتری-پرسپولیس-مقابل-استقلال-دربی-امیدها\n",
            "    Title: برتری پرسپولیس مقابل استقلال دربی امیدها\n",
            "    Cosine similarity: 0.03379732302454984\n",
            "  Document ID: 198\n",
            "    URL: https://www.farsnews.ir/news/14001221000810/تغییر-در-ساعت-دیدار-پرسپولیس-و-استقلال\n",
            "    Title: تغییر در ساعت دیدار پرسپولیس و استقلال\n",
            "    Cosine similarity: 0.03307584554723747\n",
            "  Document ID: 1048\n",
            "    URL: https://www.farsnews.ir/news/14001209001094/مدافع-پرسپولیس-بازی-مقابل-صنعت-نفت-را-از-دست-داد\n",
            "    Title: مدافع پرسپولیس بازی مقابل صنعت نفت را از دست داد\n",
            "    Cosine similarity: 0.03155935717957591\n",
            "  Document ID: 1977\n",
            "    URL: https://www.farsnews.ir/news/14001128000707/تساوی-پرگل-استقلال-و-پرسپولیس-در-رده-جوانان\n",
            "    Title: تساوی پرگل استقلال و پرسپولیس در رده جوانان\n",
            "    Cosine similarity: 0.031059282240955843\n"
          ]
        }
      ],
      "source": [
        "query = 'استقلال و پرسپولیس'\n",
        "k = 5\n",
        "execute_search_engine(query,k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: کمیسیون\n",
            "Documents:\n",
            "\n",
            "  Document ID: 9807\n",
            "    URL: https://www.farsnews.ir/news/14000923000737/نوروزی-دلخوش-و-موحد-عضو-کمیسیون-تلفیق-بودجه-۱۴۰۱-شدند\n",
            "    Title: نوروزی، دلخوش و موحد عضو کمیسیون تلفیق بودجه ۱۴۰۱ شدند\n",
            "    Cosine similarity: 0.04559174736147669\n",
            "  Document ID: 10084\n",
            "    URL: https://www.farsnews.ir/news/14000916000703/اعضای-کمیسیون-فرهنگی-به-طبس-و-رفسنجان-سفر-می‌کنند\n",
            "    Title: اعضای کمیسیون فرهنگی به طبس و رفسنجان سفر می‌کنند\n",
            "    Cosine similarity: 0.045096027601516965\n",
            "  Document ID: 9428\n",
            "    URL: https://www.farsnews.ir/news/14001005000638/قاضی‌زاده‌-طباطبایی‌نژاد-و-یزدی‌خواه-تلفیقی‌های-کمیسیون-فرهنگی-شدند\n",
            "    Title: قاضی‌زاده‌، طباطبایی‌نژاد و یزدی‌خواه تلفیقی‌های کمیسیون فرهنگی شدند\n",
            "    Cosine similarity: 0.041647699416419354\n",
            "  Document ID: 8274\n",
            "    URL: https://www.farsnews.ir/news/14001108000634/دستور-جلسات-هفته-آینده-کمیسیون‌های-مجلس\n",
            "    Title: دستور جلسات هفته آینده کمیسیون‌های مجلس\n",
            "    Cosine similarity: 0.039638450882342385\n",
            "  Document ID: 9084\n",
            "    URL: https://www.farsnews.ir/news/14001014001128/کمیسیون-تلفیق-کلیات-لایحه-بودجه-را-تصویب-کرد\n",
            "    Title: کمیسیون تلفیق کلیات لایحه بودجه را تصویب کرد\n",
            "    Cosine similarity: 0.033822020701137726\n"
          ]
        }
      ],
      "source": [
        "query = 'کمیسیون'\n",
        "k = 5\n",
        "execute_search_engine(query,k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: خودایمنی مورفه‌آ\n",
            "Documents:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = 'خودایمنی مورفه‌آ'\n",
        "k = 5\n",
        "execute_search_engine(query,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: مرض فلج اطفال\n",
            "Documents:\n",
            "\n",
            "  Document ID: 10964\n",
            "    URL: https://www.farsnews.ir/news/14000825000739/3-سازمان-متولی-موضوع-کودکان-کار-هستند-اما-آماری-از-تعداد-آنها-وجود\n",
            "    Title: 13 سازمان متولی موضوع کودکان کار هستند اما آماری از تعداد آنها وجود ندارد\n",
            "    Cosine similarity: 0.010558700088873958\n",
            "  Document ID: 7805\n",
            "    URL: https://www.farsnews.ir/news/14001120000688/انقلاب-اسلامی-اقتصاد-مقاومتی-تولید-ملی-کارآفرینی-علت-ناکامی‌ها-در\n",
            "    Title: انقلاب اسلامی، اقتصاد مقاومتی، تولید ملی، کارآفرینی/ علت ناکامی‌ها در مدیریت اقتصاد کشور، مدیران غربگرا بود\n",
            "    Cosine similarity: 0.007433801753321521\n",
            "  Document ID: 9385\n",
            "    URL: https://www.farsnews.ir/news/14001007000122/برای-کاهش-خام‌فروشی-در-دولت-قبل-اقدامی-نشد-بی‌توجهی-وزیر-سابق-نفت-به\n",
            "    Title: برای کاهش خام‌فروشی در دولت قبل اقدامی نشد/ بی‌توجهی وزیر سابق نفت به دور زدن تحریم‌ها\n",
            "    Cosine similarity: 0.003855803928056789\n",
            "  Document ID: 9386\n",
            "    URL: https://www.farsnews.ir/news/14001007000097/محورهای-تحقیق-و-تفحص-از-عملکرد-وزارت-نفت-در-دوره-سابق\n",
            "    Title: محورهای تحقیق و تفحص از عملکرد وزارت نفت در دوره سابق\n",
            "    Cosine similarity: 0.0033316009771125125\n",
            "  Document ID: 8748\n",
            "    URL: https://www.farsnews.ir/news/14001016000354/سکوت-مرگ‌بار-غربگرایان-در-مقابل-مدیریت-سنجیده-کرونا-در-کشور\n",
            "    Title: سکوت مرگ‌بار غربگرایان در مقابل مدیریت سنجیده کرونا در کشور\n",
            "    Cosine similarity: 0.0030976402343377576\n",
            "  Document ID: 9146\n",
            "    URL: https://www.farsnews.ir/news/14001012000994/فرودگاه-بغداد-ساعت-یک-و-۲۰-دقیقه-بامداد\n",
            "    Title: فرودگاه بغداد؛ ساعت یک و ۲۰ دقیقه بامداد!\n",
            "    Cosine similarity: 0.0024504512264073345\n",
            "  Document ID: 11170\n",
            "    URL: https://www.farsnews.ir/news/14000820000394/جلسات-رئیس‌جمهور-با-مجامع-استانی-نمایندگان-نتایج-خوبی-در-پی-خواهد-داشت\n",
            "    Title: جلسات رئیس‌جمهور با مجامع استانی نمایندگان نتایج خوبی در پی خواهد داشت/ دستورات آیت‌الله رئیسی برای گره‌گشایی از مشکلات\n",
            "    Cosine similarity: 0.0024077727585682133\n",
            "  Document ID: 12001\n",
            "    URL: https://www.farsnews.ir/news/14000728000521/بی‌توجهی-به-پروژه‌های-نیمه-تمام-بعضی-شهرستان‌‌‌ها-در-مصوبات-سفرهای\n",
            "    Title: بی‌توجهی به پروژه‌های نیمه تمام بعضی شهرستان‌‌‌ها در مصوبات سفرهای استانی رئیس جمهور\n",
            "    Cosine similarity: 0.00234629770941328\n",
            "  Document ID: 8927\n",
            "    URL: https://www.farsnews.ir/news/14001018001095/دبیر-سیاسی-انجمن-مستقل-دانشگاه-تهران-اتوبوس‌خوابی-نتیجه-سیاست‌های-غلط\n",
            "    Title: دبیر سیاسی انجمن مستقل دانشگاه تهران: اتوبوس‌خوابی نتیجه سیاست‌های غلط دهه هفتاد است\n",
            "    Cosine similarity: 0.0020728569989177472\n",
            "  Document ID: 10779\n",
            "    URL: https://www.farsnews.ir/news/14000830000775/مصوبه-برای-آب‌رسانی-به-43-روستای-استانی-کرمان\n",
            "    Title: مصوبه برای آب‌رسانی به 43 روستای استانی کرمان\n",
            "    Cosine similarity: 0.0015423215712227155\n",
            "  Document ID: 10971\n",
            "    URL: https://www.farsnews.ir/news/14000824000482/آبان-9|-شایعاتی-که-مانند-بنزین-روی-آتش-اعتراضات-ریخته-شد\n",
            "    Title: آبان 98| شایعاتی که مانند بنزین روی آتش اعتراضات ریخته شد\n",
            "    Cosine similarity: 0.00154016749081598\n",
            "  Document ID: 6910\n",
            "    URL: https://www.farsnews.ir/news/14001221000301/مطالبه-ما-راه-اندازی-قرارگاه‌های-جهادی-در-وزارتخانه‌ها-است\n",
            "    Title: مطالبه ما راه اندازی قرارگاه‌های جهادی در وزارتخانه‌ها است\n",
            "    Cosine similarity: 0.0014374467942773954\n",
            "  Document ID: 11705\n",
            "    URL: https://www.farsnews.ir/news/14000809000765/موافقت-هیأت-دولت-با-اختصاص-اعتبار-برای-بازسازی-خسارات-ناشی-از-زلزله-در\n",
            "    Title: موافقت هیأت دولت با اختصاص اعتبار برای بازسازی خسارات ناشی از زلزله در دو استان/ انتصاب رئیس سازمان هواپیمایی کشور\n",
            "    Cosine similarity: 0.0013309641858677054\n",
            "  Document ID: 11452\n",
            "    URL: https://www.farsnews.ir/news/14000809000484/تفاوت‌های-آبان-۱۴۰۰-رئیسی-با-آبان-۹۸-روحانی-دولت-قبل-چگونه-سر-راه\n",
            "    Title: تفاوت‌های آبان ۱۴۰۰ رئیسی با آبان ۹۸ روحانی/ دولت قبل چگونه سر راه طرح‌های ملی سنگ‌اندازی کرد؟\n",
            "    Cosine similarity: 0.0007425992750331594\n",
            "  Document ID: 11697\n",
            "    URL: https://www.farsnews.ir/news/14000804000796/پرهیز-شدید-آیت‌الله-مهدوی‌کنی-از-رانت-و-سفارش-از-علاقه-خاص-حضرت-امام\n",
            "    Title: پرهیز شدید آیت‌الله مهدوی‌کنی از رانت و سفارش/ از علاقه خاص حضرت امام(ره) تا اصرار بر تربیت دانشجو ـ سرباز برای اسلام\n",
            "    Cosine similarity: 0.00015156128689173195\n"
          ]
        }
      ],
      "source": [
        "query = 'مرض فلج اطفال'\n",
        "k = 50\n",
        "execute_search_engine(query,k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "به گزارش خبرنگار پارلمانی خبرگزاری فارس، سید مرتضی حسینی نماینده مردم میانه در نطق میان دستور خود در نشست علنی امروز (یکشنبه 21 آذر ماه) مجلس شورای اسلامی گفت: در شعارهای رئیس جمهور محترم مسأله مردمی بودن و از مردم شنیدن و در میان مردم بودن تکرار شده است، این را از دست ندهند. نماینده مردم میانه در مجلس شورای اسلامی اظهار داشت: اگر ما به معنای واقعی کلمه می خواهیم در کنار مردم باشیم، باید بی‌امان با فساد و مفسد مبارزه کنیم. بسترها و زمینه‌های فساد در قوه مجریه تشکیل می‌شود. اینجا باید با فساد مبارزه کرد. وی افزود: فرارهای مالیاتی، انحصار‌های بی‌بدیل، سوء استفاده از ارز ترجیحی، فعالیت‌های ناسالم سوداگرایانه، قاچاق و امثال اینها فسادهایی است که باید با پیگیری و با برنامه و با همت ادامه پیدا کند تا به نتیجه برسد. حسینی خاطرنشان کرد: کمبودها و مشکلات زیاد است، به ویژه مشکل مسکن، مشکل بیکاری جوانان، مشکل تورم و گرانی، مشکل قاچاق کالا و واردات بی‌رویه، اما به قول حضرت آقا ظرفیت‌ها زیادتر از مشکلات است. نماینده مردم میانه در مجلس شورای اسلامی اظهار داشت: ظرفیت‌های فراوانی در کشور وجود دارد. ما در بخش آب‌، نفت، معادن، در بازار گسترده داخلی، در مسأله همسایگان، در استعدادها و آمادگی‌های حیرت‌آور جوانان، ظرفیت‌های فوق‌العاده‌ای داریم. این ظرفیت‌ها می‌تواند بر مشکلات غلبه کند به شرط آنکه این ظرفیت‌ها درست شناخته شده و به کار گرفته شوند، از عناصر مومن، پاک و کاردان استفاده شود. مسئولان و مدیران بی‌درد و بی‌تعهد از نظام اداری و بدنه دولت پالایش می‌شوند. وی افزود: امروز ما نیازمند یک بازنگری و انقلاب اداری هستیم، هم از لحاظ گزینش و چینش مدیران هم از لحاظ اداری و بروکراسی. باید آن دسته از عناصر و مدیرانی را که در به وجود آمدن وضع موجود سهیم و شریک بوده‌اند از چرخه تصمیم‌سازی، تصمیم‌گیری و اجرایی کنار بگذاریم. حسینی خاطرنشان کرد: آقای رئیس جمهور! سال‌هاست که از نقش مخرب مافیای واردات کالا و قاچاق می‌گوییم و می‌شنویم، اما چرا حتی در دولت حضرت عالی با این پدیده خانمانسوز و ضد تولید و اشتغال به صورت جدی و قاطع مبارزه نمی‌شود. نماینده مردم میانه در مجلس شورای اسلامی اظهار داشت: سال‌هاست که بانک‌ها به صورت گسترده بنگاه داری می‌کنند، با تولید و بخش‌های مولد همراهی نمی‌کنند. پول بی‌پشتوانه خلق می‌کنند. به شدت بورس‌بازی می‌کنند، بورس سکه، بورس ارز و بورس مسکن. وی افزود: طبق آمارهای علام شده از یک میلیون و 700 هزار خانه خالی حدود یک میلیون و 400 هزار واحد آن به بانک‌ها تعلق دارد و حدود 95 درصد پاساژها و مراکز تجاری توسط شرکت‌های وابسته به بانک‌ها ساخته می‌شوند، پس چرا نظام بانکی و پولی کشور را یکبار برای همیشه اصلاح نمی‌کنید؟ سال‌هاست که از عملکرد خودروسازان که عمدتا دولتی هستند، مردم و مسئولان ناراضی‌اند و الحق هم اشکالات فراوانی در این حوزه وجود دارد. خودروی گران و بی‌کیفیت و غیرقابل رقابت با خودروهای خارجی تولید می‌کنند. خدمات پس از فروش مطلوبی ندارند، اما باز هم از آنها حمایت می‌کنیم و قدمی اساسی برای اصلاح صنعت خودروسازی کشور برنمی‌داریم. حسینی خاطرنشان کرد: آقای رئیس جمهور! همه می‌دانیم که تعلیم و تربیت اساس رشد و پیشرفت یک کشور است و معلمان رکن تعلیم و تربیت هستند و نیز می‌دانیم که جنابعالی و مجلس و اکثر قاطع نمایندگان توجه ویژه‌ای به حوزه آموزش و پرورش و حل مشکلات اقتصادی و معیشتی معلمان را دارند، اما در مرحله عمل بخش‌هایی از دولت مانند سازمان برنامه و بودجه به دلیل آنچه که کمبود اعتبار و افزایش نقدینگی ذکر می‌کنند برای تصویب لایحه رتبه‌بندی معلمان همکاری نمی‌کنند. نماینده مردم میانه در مجلس شورای اسلامی اظهار داشت: دولت می‌تواند با انجام اقداماتی نظیر حذف یارانه دهک‌های بالای جامعه، حذف حقوق‌های نجومی، اخذ مالیات‌های معوقه، اخذ مالیات از خانه‌های خالی و جلوگیری از فرار مالیاتی، اعتبار لازم برای این لایحه را تامین کند. وی در پایان با اشاره به مشکلات حوزه انتخابیه خود در بخش‌های مختلف از مسئولان خواستار رسیدگی و حل آنها شد. انتهای پیام/  \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Question 1\n",
        "\n",
        "text = documents[4092]\n",
        "tokenized_text = tokenize_text(text,my_normalizer)\n",
        "tokenized_text_for_normalizing = \" \".join(tokenized_text)\n",
        "normalized_text = my_normalizer.normalize(tokenized_text_for_normalizing)\n",
        "normalized_and_tokenized_text = tokenize_text(normalized_text,my_normalizer)\n",
        "\n",
        "\n",
        "print(text)\n",
        "print(tokenized_text)\n",
        "print(normalized_and_tokenized_text)\n",
        "changed_tokens = []\n",
        "for i,token in enumerate(normalized_and_tokenized_text):\n",
        "    if(token not in tokenized_text):\n",
        "        changed_tokens.append(token)\n",
        "\n",
        "print(changed_tokens)\n",
        "print(len(changed_tokens))\n",
        "comparing_list = []\n",
        "for token in tokenized_text:\n",
        "    normalized_token = my_normalizer.normalize(token)\n",
        "    if(normalized_token != token):\n",
        "        comparing_list.append((token,normalized_token))\n",
        "\n",
        "print(comparing_list)\n",
        "print(len(comparing_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_text = []\n",
        "for token in normalized_and_tokenized_text:\n",
        "    if(token not in stop_words):\n",
        "        filtered_text.append(token)\n",
        "\n",
        "deleted_words = []\n",
        "for token in normalized_and_tokenized_text:\n",
        "    if(token not in filtered_text):\n",
        "        deleted_words.append(token)\n",
        "print(deleted_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "verbs = my_normalizer.verbs\n",
        "stemmed_tokens_4092 = [(stemmer.convert_to_stem(token),token) for token in filtered_text]\n",
        "stemming_impact = []\n",
        "for stemmed_token,token in stemmed_tokens_4092:\n",
        "    if(stemmed_token!= token):\n",
        "        if(token in verbs):\n",
        "            stemming_impact.append((token,stemmed_token))\n",
        "print(stemming_impact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.getsizeof(positional_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_postings_list_length = sorted(positional_index.items(), key=lambda x: len(x[1][\"postings_lists\"]), reverse=True)\n",
        "print(sorted_postings_list_length[0][0],sorted_postings_list_length[1][0],sorted_postings_list_length[3][0])\n",
        "print(sorted_postings_list_length[len(sorted_postings_list_length)-1][0],sorted_postings_list_length[len(sorted_postings_list_length)-1][1][\"postings_lists\"])\n",
        "print(sorted_postings_list_length[len(sorted_postings_list_length)-2][0],sorted_postings_list_length[len(sorted_postings_list_length)-2][1][\"postings_lists\"])\n",
        "print(sorted_postings_list_length[len(sorted_postings_list_length)-3][0],sorted_postings_list_length[len(sorted_postings_list_length)-3][1][\"postings_lists\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dictionary_size = len(positional_index)\n",
        "print(dictionary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the positional index with TF-IDF values\n",
        "min_idf = 9999999999999999999999999999999999\n",
        "max_idf = -55555555555555555555555555555555\n",
        "min_word = []\n",
        "max_word = []\n",
        "for term, docs_info in positional_index.items():\n",
        "    for doc_id, _ in docs_info[\"postings_lists\"].items():\n",
        "        idf_value = len(positional_index[term][\"postings_lists\"])\n",
        "        idf = math.log(collection_size/idf_value)\n",
        "        if(idf>max_idf):\n",
        "            max_idf = idf\n",
        "            max_word = term\n",
        "        if(idf<min_idf):\n",
        "            min_idf = idf\n",
        "            min_word = term\n",
        "\n",
        "print(f\"max idf: {max_idf} in term: {max_word}\")\n",
        "print(f\"min idf: {min_idf} in term: {min_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the positional index with TF-IDF values\n",
        "min_tfidf = 9999999999999999999999999999999999\n",
        "max_tfidf = -55555555555555555555555555555555\n",
        "min_word = []\n",
        "max_word = []\n",
        "for term in stemmed_tokens:\n",
        "    tfidf = positional_index[term][\"postings_lists\"][4092][\"tf_idf\"]\n",
        "    if(tfidf>max_tfidf):\n",
        "        max_tfidf = tfidf\n",
        "        max_word = term\n",
        "    if(tfidf<min_tfidf):\n",
        "        min_tfidf = tfidf\n",
        "        min_word = term \n",
        "    \n",
        "\n",
        "print(f\"max tfidf: {max_tfidf} in term: {max_word}\")\n",
        "print(f\"min tfidf: {min_tfidf} in term: {min_word}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query:  1400 خبرنگار نمونه سال\n",
            "\n",
            "Documents:\n",
            "\n",
            "  Document ID: 1546\n",
            "    URL: https://www.farsnews.ir/news/14000928000049/لیست-ممنوعه-سال-2022-از-یازدهم-دی-ماه-اجرایی-می-شود\n",
            "    Title: لیست ممنوعه سال 2022 از یازدهم دی ماه اجرایی می شود\n",
            "    Cosine similarity: 0.7903098637920106\n",
            "\n",
            "\n",
            "  Document ID: 139\n",
            "    URL: https://www.farsnews.ir/news/14001125000182/نگاهی-به-دوپینگ-اسکی‌باز-ایران-مشکلِ-عمدی-یا-دست‌های-پشت-پرده\n",
            "    Title: نگاهی به دوپینگ اسکی‌باز ایران؛ مشکلِ عمدی یا دست‌های پشت پرده؟\n",
            "    Cosine similarity: 0.7137726749655947\n",
            "\n",
            "\n",
            "  Document ID: 4476\n",
            "    URL: https://www.farsnews.ir/news/14001008000978/بُرشی-از-کتاب-فتنه-تغلب-روایت‌هایی-از-متن-و-فرامتن-آشوب‌های-‌۸۸\n",
            "    Title: بُرشی از کتاب « فتنه تغلب»/ روایت‌هایی از متن و فرامتن آشوب‌های ‌۸۸\n",
            "    Cosine similarity: 0.6764230826423696\n",
            "\n",
            "\n",
            "  Document ID: 4517\n",
            "    URL: https://www.farsnews.ir/news/14001004000648/دیوان-محاسبات-کسری-تراز-لایحه-بودجه-سال-آینده-35درصد-از-بودجه-امسال\n",
            "    Title: دیوان محاسبات: کسری تراز لایحه بودجه سال آینده 35درصد از بودجه امسال کمتر است\n",
            "    Cosine similarity: 0.6510882971870885\n",
            "\n",
            "\n",
            "  Document ID: 4992\n",
            "    URL: https://www.farsnews.ir/news/14000917000246/مدت-اعتبار-گواهینامه-مدل‌های-مصرفی-برای-ثبت-اختراع-مشخص-شد\n",
            "    Title: مدت اعتبار گواهینامه مدل‌های مصرفی برای ثبت اختراع مشخص شد\n",
            "    Cosine similarity: 0.6388768942574329\n",
            "\n",
            "\n",
            "  Document ID: 3031\n",
            "    URL: https://www.farsnews.ir/news/14001210000092/رهبر-انقلاب-آمریکا-رژیمی-بحران‌زیست-است-که-اوکراین-را-هم-قربانی-کرده\n",
            "    Title: رهبر انقلاب: آمریکا رژیمی بحران‌زیست است که اوکراین را هم قربانی کرده\n",
            "    Cosine similarity: 0.6322478910336086\n",
            "\n",
            "\n",
            "  Document ID: 2637\n",
            "    URL: https://www.farsnews.ir/news/14001128000215/رهبر-انقلاب-در-برجام-به-برخی-نکته‌ها-توجه-نشد-و-مشکلاتی-پیش-آمد-که\n",
            "    Title: رهبر انقلاب: در برجام به برخی نکته‌ها توجه نشد و مشکلاتی پیش آمد که می‌بینیم\n",
            "    Cosine similarity: 0.5781818260608049\n",
            "\n",
            "\n",
            "  Document ID: 1554\n",
            "    URL: https://www.farsnews.ir/news/14001125001008/رشد-6-درصدی-فعالیت‌های-مرکز-پژوهش‌های-مجلس\n",
            "    Title: رشد 68 درصدی فعالیت‌های مرکز پژوهش‌های مجلس\n",
            "    Cosine similarity: 0.562417063642997\n",
            "\n",
            "\n",
            "  Document ID: 192\n",
            "    URL: https://www.farsnews.ir/news/14000921000878/آیین-نامه-ثبت-اطلاعات-اموال-غیرمنقول-دستگاه‏‌های-اجرایی-تصویب-شد\n",
            "    Title: آیین نامه ثبت اطلاعات اموال غیرمنقول دستگاه‏‌های اجرایی تصویب شد\n",
            "    Cosine similarity: 0.5098759440077784\n",
            "\n",
            "\n",
            "  Document ID: 1870\n",
            "    URL: https://www.farsnews.ir/news/14000823000705/گنجی-نباید-فقط-با-عینک-بدبینی-به-انتصابات-نگاه-کرد-مهدیار-در-گزینش\n",
            "    Title: گنجی: نباید فقط با عینک بدبینی به انتصابات نگاه کرد/ مهدیار: در گزینش و ارتقا باید شفاف باشیم\n",
            "    Cosine similarity: 0.5084785452396294\n",
            "\n",
            "\n",
            "Query:  1400 خبرنگار نمونه سال\n",
            "\n",
            "Documents:\n",
            "\n",
            "  Document ID: 662\n",
            "    URL: https://www.farsnews.ir/news/14001119000490/جلیلی-برخی-با-ظاهر-خیرخواهی-تلاش-دارند-در-مردم-تردید-ایجاد-کنند\n",
            "    Title: جلیلی: برخی با ظاهر خیرخواهی تلاش دارند در مردم تردید ایجاد کنند\n",
            "    Cosine similarity: 0.9544065705742772\n",
            "\n",
            "\n",
            "  Document ID: 1888\n",
            "    URL: https://www.farsnews.ir/news/14001219000124/تلاش-دشمن-مخفی‌کردن-پیشرفت‌ها-و-تحقیر-قهرمانان-کشور-است\n",
            "    Title: تلاش دشمن مخفی‌کردن پیشرفت‌ها و تحقیر قهرمانان کشور است\n",
            "    Cosine similarity: 0.7961644848395167\n",
            "\n",
            "\n",
            "  Document ID: 1546\n",
            "    URL: https://www.farsnews.ir/news/14000928000049/لیست-ممنوعه-سال-2022-از-یازدهم-دی-ماه-اجرایی-می-شود\n",
            "    Title: لیست ممنوعه سال 2022 از یازدهم دی ماه اجرایی می شود\n",
            "    Cosine similarity: 0.7903098637920106\n",
            "\n",
            "\n",
            "  Document ID: 139\n",
            "    URL: https://www.farsnews.ir/news/14001125000182/نگاهی-به-دوپینگ-اسکی‌باز-ایران-مشکلِ-عمدی-یا-دست‌های-پشت-پرده\n",
            "    Title: نگاهی به دوپینگ اسکی‌باز ایران؛ مشکلِ عمدی یا دست‌های پشت پرده؟\n",
            "    Cosine similarity: 0.7578309162995713\n",
            "\n",
            "\n",
            "  Document ID: 3657\n",
            "    URL: https://www.farsnews.ir/news/14001025000479/طرح-مجلس-برای-شفافیت-تسهیلات-کلان-بانکی\n",
            "    Title: طرح مجلس برای شفافیت تسهیلات کلان بانکی\n",
            "    Cosine similarity: 0.7571592166534022\n",
            "\n",
            "\n",
            "  Document ID: 2247\n",
            "    URL: https://www.farsnews.ir/news/14001004000488/تعلیق-دوباره-در-انتظار-وزنه‌برداری-مصر\n",
            "    Title: تعلیق دوباره در انتظار وزنه‌برداری مصر\n",
            "    Cosine similarity: 0.6837288144301079\n",
            "\n",
            "\n",
            "  Document ID: 4476\n",
            "    URL: https://www.farsnews.ir/news/14001008000978/بُرشی-از-کتاب-فتنه-تغلب-روایت‌هایی-از-متن-و-فرامتن-آشوب‌های-‌۸۸\n",
            "    Title: بُرشی از کتاب « فتنه تغلب»/ روایت‌هایی از متن و فرامتن آشوب‌های ‌۸۸\n",
            "    Cosine similarity: 0.6764230826423696\n",
            "\n",
            "\n",
            "  Document ID: 4992\n",
            "    URL: https://www.farsnews.ir/news/14000917000246/مدت-اعتبار-گواهینامه-مدل‌های-مصرفی-برای-ثبت-اختراع-مشخص-شد\n",
            "    Title: مدت اعتبار گواهینامه مدل‌های مصرفی برای ثبت اختراع مشخص شد\n",
            "    Cosine similarity: 0.6664132950911683\n",
            "\n",
            "\n",
            "  Document ID: 4517\n",
            "    URL: https://www.farsnews.ir/news/14001004000648/دیوان-محاسبات-کسری-تراز-لایحه-بودجه-سال-آینده-35درصد-از-بودجه-امسال\n",
            "    Title: دیوان محاسبات: کسری تراز لایحه بودجه سال آینده 35درصد از بودجه امسال کمتر است\n",
            "    Cosine similarity: 0.6510882971870885\n",
            "\n",
            "\n",
            "  Document ID: 3031\n",
            "    URL: https://www.farsnews.ir/news/14001210000092/رهبر-انقلاب-آمریکا-رژیمی-بحران‌زیست-است-که-اوکراین-را-هم-قربانی-کرده\n",
            "    Title: رهبر انقلاب: آمریکا رژیمی بحران‌زیست است که اوکراین را هم قربانی کرده\n",
            "    Cosine similarity: 0.6322478910336086\n",
            "\n",
            "\n",
            "with champion list time (micro second):\n",
            "998.4970092773438\n",
            "without champion list time (micro second):\n",
            "10975.837707519531\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_docs3(query_tf,k,flag):\n",
        "    cosine_similarities =  cosine_similarity(query_tf,final_tokens,flag)\n",
        "    sorted_cosine_similarities = sorted(cosine_similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    return sorted_cosine_similarities[:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_search_engine3(query, number_of_retrieved_docs,flag):\n",
        "    preprocessed_query = query_preprocess(query)\n",
        "    query_is_valid = is_query_valid(preprocessed_query)\n",
        "    if(query_is_valid == False):\n",
        "        print(\"Need more details\")\n",
        "        return\n",
        "    query_tf = calculate_tf_query(preprocessed_query)\n",
        "    top_answers = top_k_docs3(query_tf,number_of_retrieved_docs,flag)\n",
        "\n",
        "    show_results(query,top_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_preprocess2(query):\n",
        "    query_tokens = tokenize_text(query,my_normalizer)\n",
        "    stemmed_query = [stemmer.convert_to_stem(token) for token in query_tokens]\n",
        "    final_query = [token for token in stemmed_query if token not in stop_words]\n",
        "    return final_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_search_engine2(query, number_of_retrieved_docs):\n",
        "    preprocessed_query = query_preprocess2(query)\n",
        "    query_is_valid = is_query_valid(preprocessed_query)\n",
        "    if(query_is_valid == False):\n",
        "        print(\"Need more details\")\n",
        "        return\n",
        "    query_tf = calculate_tf_query(preprocessed_query)\n",
        "    top_answers = top_k_docs(query_tf,number_of_retrieved_docs)\n",
        "\n",
        "    show_results(query,top_answers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \".میرود\"\n",
        "print(\"with normalizing\")\n",
        "execute_search_engine(query,10)\n",
        "\n",
        "print(\"without normalizing\")\n",
        "execute_search_engine2(query,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \" 1400 خبرنگار نمونه سال\"\n",
        "k = 10\n",
        "time_s_ch = time.time()\n",
        "execute_search_engine3(query,k,True)\n",
        "time_e_ch = time.time()\n",
        "\n",
        "time_s_nch = time.time()\n",
        "execute_search_engine3(query,k,False)\n",
        "time_e_nch = time.time()\n",
        "\n",
        "print(\"with champion list time (micro second):\")\n",
        "print((time_e_ch-time_s_ch)*1000000)\n",
        "print(\"without champion list time (micro second):\")\n",
        "print((time_e_nch-time_s_nch)*1000000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fd = query_preprocess(first_doc)\n",
        "sd = query_preprocess(documents[4476])\n",
        "common_terms = []\n",
        "for term in fd:\n",
        "    if term in sd:\n",
        "        common_terms.append(term)\n",
        "\n",
        "print(len(common_terms))\n",
        "print(len(fd),len(sd))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
